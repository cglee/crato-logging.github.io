<!DOCTYPE html>
<html lang="en-US" prefix="og: http://opg.me/ns#">
  <head>
    <meta charset="UTF-8" />
    <meta name="title" property="og:title" content="Crato" />
    <meta
      name="description"
      property="og:description"
      content="Crato is an open source framework for small web applications to easily deploy a centralized logging solution that maintains ownership of data"
    />
    <meta name="type" property="og:type" content="website" />
    <meta
      name="url"
      property="og:url"
      content="https://crato-logging.github.io/"
    />
    <meta
      name="image"
      property="og:image"
      content="images/logos/crato-logo.png"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta
      name="author"
      content="Faazil Shaikh, Kurth O'Connor, Alex Soloviev"
    />

    <title>CRATO</title>
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="images/icons/favicon_package_v0.16/favicon-16x16.png"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Hind|Hind:700|Open+Sans:700|Teko:700&display=swap"
      rel="stylesheet"
    />

    <!-- <style>reset</style> -->
    <link rel="stylesheet" href="stylesheets/reset.css" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/gruvbox-dark.min.css"
      charset="utf-8"
    />

    <!-- <style></style> -->
    <link rel="stylesheet" href="stylesheets/main.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

    <!-- <script></script> -->
    <script src="javascripts/application.js"></script>
  </head>
  <body>
    <div class="logo-links">
      <p id="crato-logo">CRATO</p>

      <a href="https://github.com/crato-logging/crato" target="_blank">
        <img
          src="images/logos/github_black.png"
          alt="github logo"
          id="github-logo"
        />
      </a>
    </div>
    <nav id="site-navigation">
      <ul>
        <li>
          <a href="#home" id="home-link">HOME</a>
        </li>
        <li>
          <a href="#case-study" id="case-study-link">CASE STUDY</a>
          <nav id="case-study-mobile">
            <ul></ul>
          </nav>
        </li>
        <li>
          <a href="#our-team" id="our-team-link">OUR TEAM</a>
        </li>
      </ul>
    </nav>
    <header id="home">
      <h1>
        <img src="images/logos/crato-logo.png" alt="Crato logo" />
        <p>easy log management for small applications</p>
      </h1>
    </header>
    <section class="integration">
      <div class="box">
        <img src="images/crato_horizontal.png" alt="best practices" />
      </div>
      <article class="box">
        <div class="text-box">
          <h1>Centralize your logs</h1>
          <p>
            Crato is an open-source framework for small applications to easily
            deploy a centralized logging solution that maintains ownership of
            data.
          </p>
          <!-- <a class="button" href="#case-study">Learn More</a> -->
        </div>
      </article>
    </section>

    <main>
      <section id="case-study">
        <h1>Case Study</h1>

        <div id="side-nav">
          <img src="images/logos/crato-logo.png" alt="Crato logo" />
        </div>
        <nav>
          <ul></ul>
        </nav>

        <h2 id="introduction">1 Introduction</h2>

        <h3>1.1 What is Crato?</h3>

        <p>
          Crato is an open source framework for small applications to easily
          deploy centralized logging.
        </p>

        <p>
          Applications and system services use messages to record their events,
          but by default these log messages remain on local systems and are
          deleted. Without intervening then, developers and administrators lose
          valuable insight into the history, performance and security of their
          systems.
        </p>

        <p>
          Crato employs centralized logging to address this problem. With Crato,
          each machine sends its logs to an offsite collection server, where
          users can gain insight into the entire system by accessing the
          consolidated logs. Log messages are then streamed into Kafka, consumed
          by Crato, and sent as compressed files to Amazon S3 for archiving and
          into InfluxDB for metrics-based querying.
        </p>

        <h2 id="logging">2 Logging</h2>

        <h3>2.1 What is a Log?</h3>

        <p>
          Logs are an essential tool that developers use on a daily basis and in
          the simplest case, we can think of logs as distinct events that get
          written to files. This log data is one of the key elements in
          determining application health, as well as in analysis and debugging,
          and for deriving useful metrics. But most often, especially on a
          single node system, when there is an issue, developers can reach for
          the appropriate log file and look for errors or warnings to understand
          what went wrong. In addition to the error logs that help developers
          investigate issues, event logs across your application's services can
          also give insights on how the application is being used and are
          essential for performing historical analysis to find trends, patterns
          and user behaviours.
        </p>

        <p>
          It is natural to think of logs as files since normally, the
          environment in which our applications run include a logging system
          that separates the concern of log message processing from the
          components that generate them - one such logging system is syslog; the
          most widely distributed logging protocol that is supported by a wide
          range of devices and processes. Syslog defines a message format as
          well as allows for stanardization of how logs are generated and
          processed.
        </p>

        <h3>2.2 Logs as Files</h3>

        <p>
          Most often, especially on a UNIX machine, log files can be found in
          the <code>var/log</code> directory and apart from log files that are
          found there, this directory can contain other sub directories
          depending on applications that are running on your system. For
          example, MySQL generates its own log file and the
          <code>httpd</code> sub directory contains the apache web server
          access<em>log and error</em>log. There are also system processes that
          can be found in files like <code>kern.log</code> or
          <code>auth.log</code> that contain information logged by the kernel
          and system authorization information respectively.
        </p>
        <img src="images/diagrams/logs_as_files.png" alt="Logs as files" />
        <p>
          Because log messages are generated by some process and get written to
          files on our local file system, it's natural to think of logs simply
          as files. There are command line utilities that help developers
          interact with log files and we can use commands like
          <code>grep</code> to match against some string or regular expression
          to find what we're looking for. There is a static file, we can sift
          through it and get a result.
        </p>
        <div class="img-wrapper">
          <img
            src="images/diagrams/gifs/grep.gif"
            id="grep"
            alt="Logs as files"
          />
          <p>
            We can search through a file to look for logs generated in the last
            10 minutes.
          </p>
        </div>
        <p>
          But in order to better understand the structure of application logs
          and before exploring how log messages are generated and processed with
          syslog let's consider another description of what a log is. We know
          that a log is a file but it's not the full story.
        </p>
        <h3>2.3 Logs as Append-Only Data Structures</h3>

        <blockquote>
          <p>
            "A log is perhaps the simplest possible storage abstraction. It is
            an append-only, totally-ordered sequence of records ordered by time"
          </p>
          <p>- Jay Kreps</p>
        </blockquote>

        <p>
          In a simple case, if we have an application that runs on a single
          node, the life of the developeer is relatively easy. When the
          application is contained, so is its log data - when something goes
          wrong we know where to look and reading an error message that is
          produced can be easily parsed by a human to find out what the issue
          is. However, as applications continue to grow in complexity so does
          the volume of their log data - the many components, devices and
          microservices that produce log data are often distributed on multiple
          servers and even with a relatively small number of users, this can
          quickly become a bid data problem. For this reason there has been a
          greater need to unify log data, both by location as well as format.
          This shift means that while developers still need to access and read
          logs, this strategy becomes quickly unmanageable and so the need for
          machines to read log data instead of humans has risen drastically.
          However, in order for a machine to parse and aggregate log data to
          then be consumed by humans, a text file many not be the best format.
          So if log files are useful for humans to read, what does a log that is
          easier for machines to read look like? How do we store log data, if
          not in a file?
        </p>

        <p>
          If we take a step back and look at a log, not as a file but a data
          structure as described by Jay Kreps in the quote above, we can see
          that logs as files are just a different form of this log concept. The
          log has some basic properties:
        </p>

        <p>
          Each entry in a log is added sequentially making this an append only
          data structure. We start on the left and each subsequent entry is then
          added to the right. The entries on the left are then by definition
          older than entries on the right giving this data structure a natural
          notion of time, even without an explicit timestamp. Since reads also
          proceed from left to write, in the context of building an application,
          if each entry in a log represents some event, we can know the order in
          which the events occurred, thus building up a sort of historical
          context for our applications.
        </p>

        <div class="img-wrapper">
          <img src="images/diagrams/log_data_structure.png" alt="Log" />
        </div>

        <h3>2.4 Logs as Streams</h3>

        <div class="img-wrapper">
          <img
            src="images/diagrams/gifs/tail.gif"
            id="tail"
            alt="Logs as streams"
          />
          <p>
            Ongoing, collated collection of events which we can view in real
            time as they happen.
          </p>
        </div>

        <h3>2.5 Logs Capture Change</h3>

        <h3>2.6 Stream - Table Duality</h3>
        <div class="buttons">
          <button id="resetButton" class="cratoButton">Reset</button>
          <button id="nextButton" class="cratoButton">&rarr;</button>
        </div>
        <img
          src="images/diagrams/table_states/state1.png"
          id="table-state"
          data-id="1"
          alt="Table State1"
        />

        <img src="images/diagrams/gifs/stream.gif" alt="Stream" />

        <img
          src="images/diagrams/stream_table_duality.png"
          alt="Stream-table duality"
        />

        <h2 id="logging-for-small-apps">3 Logging for Small Apps</h2>

        <h3>Why Does Logging Matter for Small Applications?</h3>

        <p>
          Sometimes, you don't just want to know what changed; you want to know
          why or how it changed.
        </p>

        <h3>Benefits Overview</h3>

        <h3>Syslog</h3>

        <img src="images/diagrams/syslog_local.png" alt="Syslog local" />

        <img
          src="images/diagrams/syslog_local2.png"
          alt="Syslog local on client"
        />

        <img
          src="images/diagrams/gifs/local_logging.gif"
          alt="Localized logging"
        />

        <img
          src="images/diagrams/syslog_centralized.png"
          alt="Centralized syslog"
        />

        <img
          src="images/diagrams/centralized_logging.png"
          alt="Centralized logging"
        />

        <h2 id="current-solutions">4 Current Solutions</h2>

        <p>
          For a small and growing application that wants to manage their own
          logs what solutions exist?
        </p>

        <h3>The Three Options Overview</h3>

        <h3>
          LaaS: Full Service Log Management
        </h3>

        <p>LaaS Overview</p>

        <h4>
          LaaS: Log Management Components Provided
        </h4>

        <h4>LaaS Tradeoffs</h4>

        <h3 id="diy">DIY</h3>

        <p>DIY Overview</p>

        <h4>
          Log Management Components Overview
        </h4>

        <h4>Collection &amp; Aggregation</h4>

        <h5>
          Syslog Protocol &amp; its Implementations
        </h5>

        <h5>
          Collection &amp; Aggregation Options Summary
        </h5>

        <h4>Ingestion &amp; Storage</h4>

        <h4>Monitoring &amp; Visualization</h4>

        <h4>
          Log Management Components Recap
        </h4>

        <h4>DIY Tradeoffs</h4>

        <h3>OSS</h3>

        <p>OSS Overview</p>

        <h4>ElasticStack</h4>

        <h4>Graylog</h4>

        <h4>OSS Tradeoffs</h4>

        <h3>
          Summary: Current Log Management Options
        </h3>

        <h2 id="crato-design">5 Crato Design</h2>

        <p>
          The above discussion of current log management options reveals the
          niche that Crato addresses. While LaaS options provide the quickest
          and most simple path, their advantages come with costs. Not only must
          users pay for LaaS services, but their applications' data is yielded
          to a third party. DIY options grant users the most control, and can be
          designed and built to include precisely the features wanted. But there
          are tradeoffs here as well. The high level of control and
          customizability comes with increased complexity and additional design
          and maintenance committments. Finally, the pre-designed OSS options
          aim to split the difference between LaaS and DIY. OSS options return
          data control to the users (typically lacking in LaaS) while reducing
          complexity and design and maintenance commitments (both of which tend
          to be high in DIY projects).
        </p>

        <p>{chart placing Crato with OSS solutions and between DIY and LaaS}</p>

        <p>
          But even here in the OSS options, there is a range of complexity,
          which is largely attributable to the database and full-text search
          choices of the OSS stacks. Crato sits among the OSS logging solutions,
          but it reduces complexity even more by eliminating the need to set up
          and maintain a full-text search engine. Thus, Crato is designed to
          provide the core features of centralized logging while reducing
          complexity and time commitments.
        </p>

        <h3 id="51cratoarchitectureoverview">
          5.1 Crato Architecture Overview
        </h3>

        <p>
          Crato consists of two main components: the client and the central
          server. The client is installed and run on each machine from which
          logs are to be collected. The central server is deployed via Docker
          onto a separate server or VPS.
        </p>

        <p>{crato architecture diagram}</p>

        <h3 id="52commands">5.2 Commands</h3>

        <p>
          Crato client is designed to have a light footprint on each client
          system. Downloading the client adds only two files to each system: the
          <code>crato</code> bash script and <code>crato-config</code>, a text
          file to record user preferences. Running Crato client interferes with
          the native logging of the system as little as possible. Crato adds
          only a single configuration file that supplements, not overrides,
          default settings
        </p>

        <p>
          To download Crato client, <code>curl</code> or <code>wget</code> the
          script.
        </p>

        <h4 id="cratoconfig"><code>crato-config</code></h4>

        <p>
          To simplify deployment and reduce complexity, the Crato client cli
          exposes only a handful of configuration options. Crato client
          configuration options are set in the <code>crato-config</code> text
          file. Using key-value pairs, enter any number of logs for Crato to
          track, along with their tags and severity levels. Then enter the ip
          address of the central server.
        </p>

        <p>
          For users needing more granular control over settings, the full range
          of configuraton options are available by directly editing the default
          <code>*.conf</code> files. Additionally, because Crato does not
          override applications' logging processes, these can be configured as
          before.
        </p>

        <p>{Crato Client config file shot}</p>

        <h4 id="cratoc"><code>crato -c</code></h4>

        <p>
          Apply the current configuration and start Crato client with
          <code>crato -c</code> or <code>crato --configure</code>. Running this
          command checks that permissions, <code>rsyslog</code> version and base
          configuration is appropriate for Crato client. It then dynamically
          generates a <code>/etc/rsyslog.d/49-crato.conf</code> file to hold
          Crato configuration, and checks the validity of the resulting
          <code>rsyslog</code> configuration. It then either throws an error
          with instructions for debugging or (re)starts logging with the new
          settings.
        </p>

        <h4 id="cratosandcrator">
          <code>crato -s</code> and <code>crato -r</code>
        </h4>

        <p>
          <code>crato -s</code> or <code>crato --suspend</code> suspends Crato's
          logging on the client and returns the machine to default logging.
          <code>crato -r</code> or <code>crato --resume</code> restarts Crato
          logging on the client with the suspended settings resumed. If Crato
          client was not suspended at the time of execution, then the command
          makes no changes.
        </p>

        <h4 id="cratod"><code>crato -d</code></h4>

        <p>
          <code>crato -d</code> or <code>crato --delete</code> removes current
          or suspended Crato client configurations, confirms that resulting
          default configuration is valid and restarts logging with default
          settings or throws an error with instructions for debugging.
        </p>

        <p>{Crato Client CLI use gif}</p>

        <h4 id="522dockerdeployment">5.2.2 Docker Deployment</h4>

        <p>{Crato Docker Deployment gif}</p>

        <h2 id="crato-implementation">6 Crato Implementation</h2>

        <h3 id="collectionaggregation-1">Collection &amp; Aggregation</h3>

        <p>{Architecture with C&amp;A boxed}</p>

        <h3 id="61caimplementation">6.1 C &amp; A Implementation</h3>

        <p>
          Following its design mandate of being a lightweight, easy-to-use
          system, Crato collects logs using <code>rsyslog</code>, a standard
          tool available on most <code>*nix</code> systems. At the user’s
          discretion, Crato can collect any text-based log messages your
          servers, applications, databases, etc generate. Captured log messages
          are consolidated into a single stream along with system logs.
          Together, these messages are persisted to disk in the
          <code>syslog</code> file on each client machine and also routed for
          processing and forwarding.
        </p>

        <p>{inner client diagram}</p>

        <h3 id="62shippingingestion">6.2 Shipping &amp; Ingestion</h3>

        <p>{Architecture with S&amp;I boxed}</p>

        <p>
          {caption: Crato clients monitor logs and forward the log messages as
          they arrive to the core Crato infrastructure. }
        </p>

        <h4 id="621shipping">6.2.1 Shipping</h4>

        <p>
          So far, the logs are grouped into a single stream, which is being
          written to a single file. This occurs on each client (physical machine
          or virtual server). But the primary goal of centralized log management
          is to collect logs from many different sources into a single, central
          (offsite) location. To accomplish that goal, we need to ship the logs
        </p>

        <p>
          Each system running a Crato client forwards the stream of captured log
          messges via <code>tcp</code> to the offsite core of Crato's
          infrastructure. To mitigate message loss resulting from a network
          partition or problems with the central server, each Crato client
          implements a disk-assisted memory queue in front of the forwarding
          action.
        </p>

        <p>
          WHY DEFAULT TO TCP?: at our expected volumes, the disk-assisted memory
          queue (a linked list capable of holding 10000 individual log messages,
          with 1Gb disk space available if needed) should mitigate the need to
          fire and forget to keep up with traffic. Thus, Crato opts for TCP and
          its guarantees of delivery and packet order over UDP's potential speed
          gains and absence of guarantees.
        </p>

        <h4 id="622ingestion">6.2.2 Ingestion</h4>

        <p>
          As it receives new log events from the dispersed clients, the central
          log server appends each message to its own log file. At this point,
          the central collection server's <code>syslog</code> file contains all
          captured messages from the clients along with those from the
          collection server itself. The full text is available for CLI query
          using familiar text processing tools like <code>grep</code>,
          <code>tail</code>, <code>sort</code>, and <code>awk</code>.
        </p>

        <p>
          {Diagram of reading logs from across the system in a single place}
        </p>

        <p>
          {Caption: Central point for insight into entire system: all messages
          available to familiar unix tools}
        </p>

        <p>
          Central log servers also forward logs into Crato’s pub-sub system,
          where they are made available to both the archiving server and to the
          database.
        </p>

        <h3 id="63messagestreaming">6.3 Message Streaming</h3>

        <h4 id="weneedtostorelogsbuthow">We Need to Store Logs...but How?</h4>

        <p>
          It is common practice for log management systems to persist log data
          not only to save it from automatic log rotation jobs, but also to
          facilitate metrics and other system analysis. Crato follows this
          practice, with Amazon S3 Glacier used for long-term storage and
          InfluxDB used for metrics and eventual visualization.
        </p>

        <p>
          Couldn't Crato simply send logs directly to the persistence layer?
          Yes, in fact it's earliest iterations did exactly that, but this
          approach is sub-optimal for a number of related reasons.
        </p>

        <p>
          Logs are produced at different rates than they are consumed.
          Furthermore, the same log data will be used by multiple processes.
          Each use can be expected to have different needs, to operate at a
          different rate, and to have potentially different bottlenecks or
          failure rates. As a result, sending a copy of each log message to each
          consumer as they come in is likely to lead to dropped messages with no
          good way of recovery. Crato needed a buffering system to capture logs
          and allow each consumer to draw data at its own rate.
        </p>

        <p>
          One way to address this problem would be to use a queue. Log messages
          are sent into a queue, and a consumer shifts the next message off as
          needed. This may would work well for an individual consumer, but Crato
          needs to handle multiple consumers, and as each consumer grabs the
          next available message from the queue, all the others miss out. The
          message is no longer available. What Crato would need would be a queue
          in front of each potential consumer. To implement that, the central
          server would need to send a copy of each log message to n queues in
          front of n consumers.
        </p>

        <p>
          Another potential solution would be to use a publish-subscribe
          messaging system. The central server would publish each new log
          message to the platform, and all subscribers would receive the log as
          it is available. While this model allows the central server to forward
          its stream to a single destination, it does not account for the
          different rates of consumption.
        </p>

        <h4 id="631decouplingloggenerationusewithkafka">
          6.3.1 Decoupling Log Generation &amp; Use with Kafka
        </h4>

        <p>{Architecture with Streaming / Kafka boxed }</p>

        <p>
          What Crato needs is a way to provide the same stream of log messages
          to multiple consumers, to allow producers and consumers to work at
          their own paces, and for each to always be able to resume its work at
          the exact point it left off. It needs the advantages of a queue and a
          pub-sub system folded into a single platform. Apache Kafka provides
          exactly that.
        </p>

        <blockquote>
          <p>
            The consumer group concept in Kafka generalizes these two concepts.
            As with a queue the consumer group allows you to divide up
            processing over a collection of processes (the members of the
            consumer group). As with publish-subscribe, Kafka allows you to
            broadcast messages to multiple consumer groups.
            <a href="https://kafka.apache.org/documentation.html#kafka_mq"
              >Kafka as a Messaging System</a
            >
          </p>
        </blockquote>

        <p>
          By decoupling log creation or ingestion from log use, Kafka solves the
          problem noted above, namely Crato's need for exactly once delivery of
          log messages to multiple users operating at different paces. At the
          same time, it also allows Crato to be more extensible in the future.
          Any additional uses for the same data can be added as consumers.
        </p>

        <p>
          But if Kafka is so effective at managing this decoupling, wouldn't
          Crato benefit from skipping the central server model and simply have
          each client produce directly to remote Kafka brokers instead? There
          are advantages to this approach. It would reduce complexity in Crato's
          core infrastructure, and there is no doubt that Kafka could handle the
          load from multiple distributed clients as well as it does from a
          single central log server.
        </p>

        <p>
          That said, this design would increase the complexity on the client
          side by introducing additional configuration requirements and
          dependencies. If we want to use the <code>syslog</code> protocol on
          each client (see the justification above), then an
          <code>rsyslog</code> central server poses fewer problems for client’s
          installation. Also, removing the central server would eliminate one of
          Crato's UI advantages, because without a central collection point it
          would lose the ability to provide system-wide observability using the
          familiar *nix text processing tools. One of the design goals is 'easy
          to use'. Asking users to query Kafka for observability pushes hard
          against that goal.
        </p>

        <h4 id="producingtokafka">Producing to Kafka</h4>

        <p>
          "Crato's central collection server sends logs to Kafka via rsyslog's
          Omkafka module."
        </p>

        <p>DA memory queue to mitigate server / broker partition</p>

        <h4 id="consumingfromkafka">Consuming from Kafka</h4>

        <p>Kafka provides exactly once delivery for multiple consumers</p>

        <p>Consumers must still manages flow to suit endpoint needs</p>

        <h4 id="consumingfromkafkaimplementation">
          Consuming from Kafka: Implementation
        </h4>

        <p>
          Batching: archiving consumer saves log to a json files &amp; sends to
          S3
        </p>

        <p>
          Streaming: the influxDB consumer filteres and sends incoming messages
          to their appropriate measurements
        </p>

        <h3 id="storage">Storage</h3>

        <h4 id="dualstorageneeds">Dual Storage Needs</h4>

        <p>
          Archives: raw log files saved for as long as needed delayed access is
          acceptable
        </p>

        <p>
          Metrics-based Query: immediate access to recent log messages track
          entities over time
        </p>

        <h4 id="archivesamazons3glacier">Archives: Amazon S3 Glacier</h4>

        <p>
          Offsite storage 99.99...%. Durability Automatic lifecycle management
          Queryable from AWS console Inexpensive (from $0.004 / GB)
        </p>

        <p>
          Durability: expected survival percentage of an object in S3 over a
          year
        </p>

        <p>
          Another PRO of using S3: we can use another AWS service Athena to
          query our data using SQL language
        </p>

        <p>
          “Amazon Athena is an interactive query service that makes it easy to
          analyze data in Amazon S3 using standard SQL. Athena is serverless, so
          there is no infrastructure to manage, and you pay only for the queries
          that you run.”
        </p>

        <p>
          This gives us search and analytics capabilities over our data directly
          in the AWS console.
        </p>

        <p>
          TRANSITION: To understand our choice for immediate access db, we need
          to look first at the precise needs addressed by this component
        </p>

        <h4 id="queryablestorageneeds">Queryable Storage Needs</h4>

        <p>
          Track applications &amp; services as they change across time Full text
          search of log messages is not necessary Examples Patterns in server
          responses Follow count of logs with ‘err’ severity level by unit of
          time
        </p>

        <p>
          No need for full text search means tools like ElasticSearch are not
          ideal (thus that a default and often difficult to manage component of
          Graylog can be avoided) E.g. patterns in server responses: server 1
          has handled 2000 requests on average each hour; last hour it responded
          to 0 TRANSITION: these needs reveal that here, Crato is working with
          logs as time series data
        </p>

        <h4 id="timeseriesdatabases">Time-Series Databases</h4>

        <p>
          Time Series Data: sequences of data points from individual sources
          over time
        </p>

        <ul>
          <li>Time is one axis when TS data is plotted on a graph</li>

          <li>Relationships between entities (tables) not important</li>
        </ul>

        <p>
          Time Series Databases: databases built for collecting, storing,
          retrieving &amp; processing TS data
        </p>

        <ul>
          <li>
            Retention policy / aggregation of fine-grained data as it ages
          </li>

          <li>Query &amp; summarization tools</li>

          <li>Integration with metrics visualization tools</li>
        </ul>

        <p>
          Crato’s needs for queryable storage show that it deals with TS data;
          although other forms of DB can be used TS data, they lack the tooling
          to make working with TS data easy --other DB types require more coding
          overhead-- Continuous query over a short retention period measurement
          allows, for example, live monitoring of a server response rate TSDB
          tend to be optimized for fast indexing by time; While trad db slow
          down due to indexing time as the data set grows, TSDB tend to keep a
          consistently high ingestion rate TRANSITION: once we narrow it down to
          a time series database, we went with influx ...
        </p>

        <h4 id="queryablestorageinfluxdb">Queryable Storage: InfluxDB</h4>

        <p>
          Schema-free: measurements created automatically for each log source
          InfluxQL is like SQL
        </p>

        <p>
          Since most TSDB have similar optimizations for TSD, and they tend to
          be immersed in an ecosystem that allows easy integration with
          visualization tools, our decision came down to the schema-free design
          of influx. Although Timescale appears to ingest more quickly at high
          volumes, Crato’s consumer has Kafka in front of it, so ingestion speed
          can be managed, &amp; Crato’s volume is not designed for those rates
          anyway (? 10K+ ? per second)
        </p>

        <h2 id="challenges">7 Challenges</h2>

        <h3 id="71collectlogs">7.1 Collect Logs</h3>

        <p>Common collection options not chosen by Crito:</p>

        <p>
          SCALING CRATO: Crato scales horizontally with the growth of the
          application; simply configure each new server for Crato collection.
        </p>

        <h3 id="72producingtokafka">7.2 Producing to Kafka</h3>

        <h3 id="73docker">7.3 Docker</h3>

        <p>
          crashed broker causes problems through out pipeline. Must identfy and
          orchestrate restart sequence.
        </p>

        <h2 id="future-plans">8 Future Plans</h2>

        <p>Additional visualization integration</p>

        <p>Security</p>

        <ul>
          <li>option for LAN -client-server communication with TLS</li>
        </ul>

        <p>Compression option for large messages</p>
        <section id="footnotes">
          <h2 id="references">9 References</h2>

          <h5>9.1 Footnotes</h5>
          <ol>
            <li id="footnote-1">
              <a href="" target="_blank"></a>
            </li>
          </ol>
          <!--
          <h5>9.2 Resources</h5>
          <ol>
            <li><a
                href="https://www.manning.com/books/serverless-architectures-on-aws"
                target="_blank">Serverless Architecture on AWS</a>
            </li>
          </ul>
-->
        </section>
      </section>
    </main>
    <section id="our-team">
      <h1>Our Team</h1>
      <p>
        We are looking for opportunities. If you liked what you saw and want to
        talk more, please reach out!
      </p>
      <ul>
        <li class="individual">
          <img src="https://github.com/4g3m.png?" alt="Faazil Shaikh" />
          <h3>Faazil Shaikh</h3>
          <p>San Francisco, CA</p>
          <ul class="social-icons">
            <li>
              <a href="mailto:faazil.shaikh@gmail.com" target="_blank">
                <img src="images/icons/email_icon.png" alt="email" />
              </a>
            </li>
            <li>
              <a href="https://" target="_blank">
                <img src="images/icons/website_icon.png" alt="website" />
              </a>
            </li>
            <li>
              <a href="https://" target="_blank">
                <img src="images/icons/linked_in_icon.png" alt="linkedin" />
              </a>
            </li>
          </ul>
        </li>
        <li class="individual">
          <img
            src="https://github.com/jkurthoconnor.png"
            alt="Kurth O'Connor"
          />
          <h3>Kurth O'Connor</h3>
          <p>New York, NY</p>
          <ul class="social-icons">
            <li>
              <a href="mailto:jkurthoconnor@gmail.com" target="_blank">
                <img src="images/icons/email_icon.png" alt="email" />
              </a>
            </li>
            <li>
              <a href="http://kurthoconnor.com" target="_blank">
                <img src="images/icons/website_icon.png" alt="website" />
              </a>
            </li>
            <li>
              <a
                href="https://www.linkedin.com/in/kurth-o-connor-01986a169"
                target="_blank"
              >
                <img src="images/icons/linked_in_icon.png" alt="linkedin" />
              </a>
            </li>
          </ul>
        </li>
        <li class="individual">
          <img src="https://github.com/alex-solo.png" alt="Alex Soloviev" />
          <h3>Alex Soloviev</h3>

          <p>Toronto, Canada</p>
          <ul class="social-icons">
            <li>
              <a href="mailto:alex.soloviev@gmail.com" target="_blank">
                <img src="images/icons/email_icon.png" alt="email" />
              </a>
            </li>
            <li>
              <a href="https://" target="_blank">
                <img src="images/icons/website_icon.png" alt="website" />
              </a>
            </li>
            <li>
              <a href="https://www.linkedin.com/" target="_blank">
                <img src="images/icons/linked_in_icon.png" alt="linkedin" />
              </a>
            </li>
          </ul>
        </li>
      </ul>
    </section>
  </body>
</html>
